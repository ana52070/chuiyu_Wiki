import requests
from bs4 import BeautifulSoup
import html2text
import os
import time
import re

# ================= é…ç½®åŒºåŸŸ =================
CSDN_ID = "chui_yu666"  # ä½ çš„ CSDN ID
TARGET_DIR = "./blog"   # æ–‡ç« ä¿å­˜ç›®å½•
# ===========================================

# ä¼ªè£…æˆæµè§ˆå™¨ï¼Œé˜²æ­¢è¢«åçˆ¬
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def get_article_list(user_id):
    """è·å–ç”¨æˆ·æ‰€æœ‰æ–‡ç« é“¾æ¥"""
    links = []
    page = 1
    print(f"ğŸ•·ï¸ å¼€å§‹æ‰«æç”¨æˆ· {user_id} çš„æ–‡ç« åˆ—è¡¨...")
    
    while True:
        url = f"https://blog.csdn.net/{user_id}/article/list/{page}"
        try:
            resp = requests.get(url, headers=HEADERS)
            if resp.status_code != 200:
                break
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            article_list = soup.find_all('div', class_='article-item-box')
            
            if not article_list:
                break # æ²¡æœ‰æ›´å¤šæ–‡ç« äº†
            
            found_new = False
            for item in article_list:
                link_tag = item.find('a')
                if link_tag:
                    href = link_tag['href']
                    # è¿‡æ»¤æ‰éæ–‡ç« é“¾æ¥
                    if "/article/details/" in href:
                        links.append(href)
                        found_new = True
            
            if not found_new:
                break
                
            print(f"   å·²æ‰«æç¬¬ {page} é¡µï¼Œç´¯è®¡å‘ç° {len(links)} ç¯‡æ–‡ç« ")
            page += 1
            time.sleep(10) # ç¤¼è²Œçˆ¬è™«ï¼Œæ­‡ä¸€ç§’
            
        except Exception as e:
            print(f"âŒ æ‰«æåˆ—è¡¨å‡ºé”™: {e}")
            break
            
    # å»é‡
    return list(set(links))

def parse_article(url):
    """è§£æå•ç¯‡æ–‡ç« å†…å®¹"""
    try:
        resp = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 1. è·å–æ ‡é¢˜
        title_tag = soup.find('h1', id='articleContentId')
        title = title_tag.get_text().strip() if title_tag else "æ— æ ‡é¢˜æ–‡ç« "
        
        # 2. è·å–å‘å¸ƒæ—¶é—´
        date_tag = soup.find('span', class_='time')
        date = date_tag.get_text().strip() if date_tag else ""
        # ç®€å•å¤„ç†æ—¥æœŸæ ¼å¼ï¼Œåªå– YYYY-MM-DD
        if " " in date:
            date = date.split(" ")[0]
            
        # 3. è·å–æ­£æ–‡ HTML
        content_div = soup.find('div', id='content_views')
        if not content_div:
            return None
            
        # 4. è½¬æ¢ä¸º Markdown
        # é…ç½® html2text
        converter = html2text.HTML2Text()
        converter.ignore_links = False
        converter.ignore_images = False
        converter.body_width = 0 # ä¸è‡ªåŠ¨æ¢è¡Œ
        converter.protect_links = True
        
        markdown_content = converter.handle(str(content_div))
        
        return {
            "title": title,
            "date": date,
            "content": markdown_content,
            "url": url
        }
    except Exception as e:
        print(f"âŒ è§£ææ–‡ç« å¤±è´¥ {url}: {e}")
        return None

def save_to_markdown(data):
    """ä¿å­˜ä¸º MD æ–‡ä»¶"""
    # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    safe_title = re.sub(r'[\\/*?:"<>|]', "", data['title'])
    safe_title = safe_title.replace(" ", "_") # ç©ºæ ¼è½¬ä¸‹åˆ’çº¿
    
    filename = f"{safe_title}.md"
    filepath = os.path.join(TARGET_DIR, filename)
    
    # æ„å»º Frontmatter (VitePress éœ€è¦çš„å¤´éƒ¨)
    frontmatter = f"""---
title: {data['title']}
date: {data['date']}
tags: [CSDNæ¬è¿]
---

# {data['title']}

> åŸæ–‡é“¾æ¥ï¼š[{data['title']}]({data['url']})

{data['content']}
"""
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(frontmatter)
    
    print(f"âœ… å·²ä¿å­˜: {filename}")

def main():
    # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(TARGET_DIR):
        os.makedirs(TARGET_DIR)
        
    # 2. è·å–æ‰€æœ‰é“¾æ¥
    article_links = get_article_list(CSDN_ID)
    print(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(article_links)} ç¯‡æ–‡ç« ï¼Œå‡†å¤‡å¼€å§‹æ¬è¿...\n")
    
    # 3. éå†ä¸‹è½½
    for i, link in enumerate(article_links):
        print(f"[{i+1}/{len(article_links)}] æ­£åœ¨å¤„ç†: {link}")
        article_data = parse_article(link)
        if article_data:
            save_to_markdown(article_data)
            time.sleep(15) # æ¯ç¯‡æ–‡ç« é—´éš” 1.5 ç§’ï¼Œé˜²æ­¢ IP è¢«å°
            
    print("\nğŸ‰ å…¨éƒ¨æ¬è¿å®Œæˆï¼å¿«å»è¿è¡Œ npm run docs:dev çœ‹çœ‹æ•ˆæœå§ï¼")

if __name__ == "__main__":
    main()